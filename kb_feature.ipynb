{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd1e76b4",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24e9763",
   "metadata": {},
   "source": [
    "## The project\n",
    "\n",
    "Our project focuses on the Minihack environment called HideNSeek. This environment consists of a labyrinth with trees and clouds. The agent's goal is to reach the stairs without being killed by the monster. There are different versions of this environment, but our project primarily concentrates on the Mapped version, where the environment is fully observable, and the standard version of HideNSeek, where the environment is partially observable and revealed to the agent as it explores the map. Our project is structured around two distinct approaches: a knowledge base and the A* pathfinding algorithm. Initially, both techniques were explored in parallel. For both, we started with a simplified version of the problem, beginning with a fully observable environment. Initially, the map only contained trees, and later clouds and monsters were included.\n",
    "\n",
    "Subsequently, we integrated the two strategies in different ways, which will be detailed later. Finally, we transitioned to the partially observable version of the problem, where we once again implemented a strategy that combined both techniques.\n",
    "\n",
    "## Monsters\n",
    "\n",
    "Different monsters in Nethack have different characteristics:\n",
    "- Some monsters are slower than the agent, thus they occasionally skip turns;\n",
    "- Many of the monsters have (powerful) ranged attacks;\n",
    "- Some monsters have an ability called *infravision*, which means that they can locate the agent no matter the distance or obstacles;\n",
    "\n",
    "Even though the agent has a leather armor, enabling him to endure hits, many of the monsters in the environment are too powerful to confront with an empty inventory.\n",
    "As a consequence, we decided to focus on a subset of them, considering only two monsters: the Naga and the Giant Humanoid. The 'Hide and Seek' environment's DES file has been appropriately modified to include only these two creatures.\n",
    "Both the Naga and the Giant can be lethal both up close and from a distance, but they have their respective strengths and weaknesses. The fact that these two monsters are quite different from each other, allowed us to test our approach effectively on a manageable scale. \n",
    "The Naga is a serpent-like creature known for its agility and speed. It is a very fast monster, and its long-range attack is more potent, making it a significant threat when engaged from a distance. \n",
    "The Giant is a large humanoid creature, known for its brute strength and durability. It is slower but inflicts substantial damage when in close proximity. This makes it particularly dangerous if the player gets too close. \n",
    "\n",
    "\n",
    "## Stats\n",
    "The empirical evaluation is ran over 500 episodes, with a maximum number of steps of 30, using the default Minihack Reward Manager, which yields a positive score of 1 for a successful episode, and 0 otherwise. We acquired statistics over different monster pools:\n",
    "- The *full* HideNSeek pool (**Giant, Naga, Titan, Dragon, Ettin, Minotaur, Lich, Ogre, Troll**);\n",
    "- **Giant** only;\n",
    "- **Naga** only.\n",
    "\n",
    "We plotted the number of successful episodes (win/losses), the number of deaths caused by each monster (for the full pool), and the execution time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb1dd96",
   "metadata": {},
   "source": [
    "# The Fully Observable Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a2f94-dc9a-4718-9ebe-2bd5bb3d2b30",
   "metadata": {},
   "source": [
    "# Knowledge Base Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "204b33d2-38fd-4854-979e-b97381859601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyswip import Prolog\n",
    "import IPython.display as display\n",
    "from project_utils import *\n",
    "from map_utils import *\n",
    "from search import *\n",
    "from graphics_utils import *\n",
    "\n",
    "labels = {\n",
    "    \"N\": 'Naga',\n",
    "    \"H\": 'Gigant',\n",
    "    \"L\": 'Liches',\n",
    "    \"O\": 'Ogre',\n",
    "    \"T\": 'Troll',\n",
    "    \"D\": 'Dragon',\n",
    "    None: 'Unknown',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d399c746-747e-4fc7-9550-e69efa57a09a",
   "metadata": {},
   "source": [
    "### Basic Movements\n",
    "\n",
    "The first thing we did, after setting up the environment and finding out a way to formulate and update perceptions from the game into the knowledge base, was to design and test the basic movement towards the goal. Here's a test of the agent's movement in a fully mapped environment, with just trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e357fe-d597-459b-876f-b339e4208341",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ENV = gym.make(\"MiniHack-Navigation-Custom-v0\",des_file=\"dat/kb_movement.des\",\n",
    "              observation_keys=('screen_descriptions','message','pixel','blstats'))\n",
    "\n",
    "evaluate(num_ep = 1, max_steps = 30, kb_path = 'kbs/looping_kb.pl', env = ENV, speed = \"slow\", show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a99a17-2199-4290-91b5-b3959d01c3b3",
   "metadata": {},
   "source": [
    "### Loops\n",
    "Since the project's bigger picture would have included a heuristic pathfinding solution, we kept the reasoning \"step-by-step\" in the Knowledge Base version, without using lists to build a path before moving. For this reason, in this version of the system, loops might occur: it can be the case that the optimal path is blocked by trees or/and walls, and the agent walks back and forth over the same two cells even if these moves lead to a dead end. In the next map we can see an example of this behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120b99b1-172e-4c37-af31-ae8d31944e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = gym.make(\"MiniHack-Navigation-Custom-v0\",des_file=\"dat/kb_loop.des\",\n",
    "              observation_keys=('screen_descriptions','message','pixel','blstats'))\n",
    "evaluate(num_ep = 1, max_steps = 6, kb_path = 'kbs/looping_kb.pl', env = ENV, speed = \"slow\", show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02422c73-6431-4b05-a291-018eb96a8459",
   "metadata": {},
   "source": [
    "### Loops fix\n",
    "To avoid the occurrence of loops, we \"discouraged\" the agent from walking over already-walked cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac18290-4002-4bd0-a556-888f16bc6f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = gym.make(\"MiniHack-Navigation-Custom-v0\",des_file=\"dat/kb_loop.des\",\n",
    "              observation_keys=('screen_descriptions','message','pixel','blstats'))\n",
    "evaluate(num_ep = 1, max_steps = 30, kb_path = 'kbs/project_kb.pl', env = ENV, speed = \"slow\", show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b828a5-379b-4749-94d7-413f3d5ee8be",
   "metadata": {},
   "source": [
    "### Introducing monsters\n",
    "Now it's time to bring the monsters in.\n",
    "In this version, monsters are not considered by the agent at all: he just rushes towards the stairs, ignoring them. This didn't lead to horrible results though, because being exposed for a long time or avoiding close contact with the enemy can be counterproductive.\n",
    "Though not catastrophic, this strategy isn't enough, especially against the **Naga**. She rocks a powerful fire ranged attack, whilst having the same speed as the agent. Since enemies can attack us at range from basically *any* distance, performing intricated movement strategies would both put us at risk of exceeding the maximum number of steps and give the monster more attempts to hit us at range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = gym.make(\"MiniHack-HideNSeek-Mapped-v0\",\n",
    "              observation_keys=('screen_descriptions','message','pixel','blstats'))\n",
    "evaluate(num_ep = 3, max_steps = 30, kb_path = 'kbs/looping_kb.pl', env = ENV, speed = \"slow\", show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628e0c5b",
   "metadata": {},
   "source": [
    "### Evaluation \n",
    "Now we look at some evaluation, with different monster pools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347202fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = gym.make(\"MiniHack-HideNSeek-Mapped-v0\",\n",
    "              observation_keys=('screen_descriptions','message','pixel','blstats'))\n",
    "evaluate(num_ep = 500, max_steps = 30, kb_path = 'kbs/looping_kb.pl', env = ENV, speed = \"fast\", show = False)\n",
    "\n",
    "#ADD GRAPH\n",
    "#REMOVE IMPOSSIBLE TO PERFORM ANY ACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa64d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = gym.make(\"MiniHack-Navigation-Custom-v0\", des_file=\"dat/fully_observable_N.des\",\n",
    "              observation_keys=('screen_descriptions','message','pixel','blstats'))\n",
    "evaluate(num_ep = 500, max_steps = 30, kb_path = 'kbs/looping_kb.pl', env = ENV, speed = \"fast\", show = False)\n",
    "\n",
    "#ADD GRAPH\n",
    "#REMOVE IMPOSSIBLE TO PERFORM ANY ACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b65f68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = gym.make(\"MiniHack-Navigation-Custom-v0\", des_file=\"dat/fully_observable_H.des\",\n",
    "              observation_keys=('screen_descriptions','message','pixel','blstats'))\n",
    "evaluate(num_ep = 500, max_steps = 30, kb_path = 'kbs/looping_kb.pl', env = ENV, speed = \"fast\", show = False)\n",
    "\n",
    "#ADD GRAPH\n",
    "#REMOVE IMPOSSIBLE TO PERFORM ANY ACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc11f482",
   "metadata": {},
   "source": [
    "### Dealing with monsters\n",
    "When dealing with monsters, one useful thing is that ranged attacks seem to have a *lower* chance to connect if the line of fire is not exactly *perpendicular* (or *diagonal*). Thus, we made the agent less prone to walk in those cells, as well as the cells strictly adjacent to the monster. Also, the agent keeps tracks of the last enemy position, when it disappears into clouds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7918553",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = gym.make(\"MiniHack-Navigation-Custom-v0\", des_file=\"dat/fully_observable_NH.des\",\n",
    "              observation_keys=('screen_descriptions','message','pixel','blstats'))\n",
    "evaluate(num_ep = 3, max_steps = 30, kb_path = 'kbs/project_kb.pl', env = ENV, speed = \"slow\", show = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f014c9",
   "metadata": {},
   "source": [
    "### Evaluation II\n",
    "Again, some evaluation on the different monster pools. The Knowledge Base tweaks brought an improvement in all of the three monster pools performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7e079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = gym.make(\"MiniHack-HideNSeek-Mapped-v0\",\n",
    "              observation_keys=('screen_descriptions','message','pixel','blstats'))\n",
    "evaluate(num_ep = 500, max_steps = 30, kb_path = 'kbs/project_kb.pl', env = ENV, speed = \"fast\", show = False)\n",
    "\n",
    "#ADD GRAPH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762fcdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = gym.make(\"MiniHack-Navigation-Custom-v0\", des_file=\"dat/fully_observable_N.des\",\n",
    "              observation_keys=('screen_descriptions','message','pixel','blstats'))\n",
    "evaluate(num_ep = 500, max_steps = 30, kb_path = 'kbs/project_kb.pl', env = ENV, speed = \"fast\", show = False)\n",
    "\n",
    "#ADD GRAPH\n",
    "#REMOVE IMPOSSIBLE TO PERFORM ANY ACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b16efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = gym.make(\"MiniHack-Navigation-Custom-v0\", des_file=\"dat/fully_observable_H.des\",\n",
    "              observation_keys=('screen_descriptions','message','pixel','blstats'))\n",
    "evaluate(num_ep = 500, max_steps = 30, kb_path = 'kbs/project_kb.pl', env = ENV, speed = \"fast\", show = False)\n",
    "\n",
    "#ADD GRAPH\n",
    "#REMOVE IMPOSSIBLE TO PERFORM ANY ACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25948ac2",
   "metadata": {},
   "source": [
    "# Heuristic Search "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1736cb21",
   "metadata": {},
   "source": [
    "The idea was to approach the problem in an increasing level of difficolty fashion, first working on some simpler heuristics and cost functions, up to reaching a more sofisticated level of complexity. \n",
    "\n",
    "We started by comparing the Euclidean Distance and the Chebyshev Distance, to better understand how to approach the problem.  After a brief analysis, we realized that the Chebyshev Distance was exactly what we needed, because it provided eight normal vectors that accurately matched the possible actions of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453ee310",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MiniHack-Navigation-Custom-v0\", observation_keys=(\"chars\", \"pixel\", \"colors\"), des_file = \"dat/simple_maze.des\")\n",
    "solve_and_plt(env, heuristic=euclidean_distance, precision=\"base\", plt_width=(100, 270), plt_height=(500, 760))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123688ea",
   "metadata": {},
   "source": [
    "Initially, a simpler version of the problem was tackled. An A* algorithm was implemented that could find the optimal path in a map with trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7825506",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MiniHack-Navigation-Custom-v0\", observation_keys=[\"chars\", \"pixel\", \"colors\"], des_file=\"dat/square_trees.des\")\n",
    "solve_and_plt(env, heuristic=chebyshev_distance, precision=\"base\", plt_width=(100, 270), plt_height=(500, 760))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c0df85",
   "metadata": {},
   "source": [
    "The algorithm also works on more intricate map configurations, that are not constrained to a square shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f99ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MiniHack-Navigation-Custom-v0\", observation_keys=[\"chars\", \"pixel\", \"colors\"], des_file=\"dat/maze_trees.des\")\n",
    "solve_and_plt(env, heuristic=chebyshev_distance, precision=\"base\", plt_width=(50, 270), plt_height=(500, 760))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8f1471",
   "metadata": {},
   "source": [
    "The problem was made more complex with the introduction of clouds and a \"grid bug,\" a relatively weak monster in Nethack. This creature, matching the player's speed, is unable to ever catch up with the agent, adding a strategic element to the scenario. \n",
    "\n",
    "To effectively face this issue, a dynamic A* algorithm was implemented. This solution recalculates the path based on whether the monster is visible or not. In scenarios where the monster is visible, the agent incurs no penalties traversing through clouded areas. Instead, in instances where the monster remains hidden, traversing through clouded regions carries a risk, as these areas might serve as secret hiding places for the lurking monster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab030d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MiniHack-Navigation-Custom-v0\", observation_keys=[\"chars\", \"pixel\", \"colors\"], des_file=\"dat/grid_bug.des\")\n",
    "solve_and_plt(env, heuristic=chebyshev_distance, precision=\"base\", plt_width=(100, 270), plt_height=(500, 760))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c67fae",
   "metadata": {},
   "source": [
    "The final stage of the fully observable problem involved addressing the comprehensive challenge at hand. A diverse array of monsters was integrated into the scenario. \n",
    "\n",
    "Initially, we relied solely on the strategy discussed up to this point, but we soon realized that it required further refinement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaa0b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MiniHack-HideNSeek-Mapped-v0\", observation_keys=[\"chars\", \"pixel\", \"colors\"])\n",
    "solve_and_plt(env, heuristic=chebyshev_distance, precision=\"base\", plt_width=(100, 270), plt_height=(500, 760))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473a9edb",
   "metadata": {},
   "source": [
    "The idea behind the improved strategy roots in two simple concepts:\n",
    "- when the agent has the chance to see the monster in the map, it should keep itself further from it\n",
    "- when the agent does not have a chance to get to know where the monster may be hiding itself, the agent must stay further from clouds as much as it might, for this very reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ac85b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MiniHack-HideNSeek-Mapped-v0\", observation_keys=[\"chars\", \"pixel\", \"colors\"])\n",
    "solve_and_plt(env, heuristic=chebyshev_distance, precision=\"advanced\", plt_width=(100, 270), plt_height=(500, 760), dynamic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f60c8bf",
   "metadata": {},
   "source": [
    "Let us give a brief discussion on the reasons behind the procedure rightfully working.\n",
    "\n",
    "First, we will introduce the formulation of the Chebyshev Distance, i.e:\n",
    "$$\\text{let} \\quad P = (p_1, p_2), \\quad \\text{and} \\quad Q = (q_1, q_2)$$\n",
    "$$d(P, Q) = \\max(|p_1 - q_1|, |p_2 - q_2|)$$\n",
    "\n",
    "The idea, as expressed before in more abstract terms, is to keep the agent as far as possible from the monster whenever it is visible. This can be obtained by setting the $g$ function of the cost function $f$ in such a way that it weights cells closer to the monster more than cells further from it. \n",
    "\n",
    "Let us first define the cost function $f$:\n",
    "$$f(P, T) := g(P, D) + h(P, T),$$\n",
    "$$\\text{where P is the neighbour position},$$\n",
    "$$\\text{D is the danger position},$$\n",
    "$$\\text{and T is the target position}$$\n",
    "\n",
    "Note: the definition of the function $g$ actually needs to be taken in a more flexible way since, as we will see later, it will be having parameters of different types throughout all the instances of the problem. Specifically, $D$ should be considered as a set, and $g$ musti be defined with a specific formulation.\n",
    "\n",
    "Anyway, what we mentioned above can be obtained by a simple manipulation of the function we used for the calculation of the distance between points in the plane, i.e. by simply calculating the reciprocal of the Chebyshev Distance between the position and the monsters itself. This gives us a function which is \"rotation aware\": it assigns the same value to positions falling in the radius of a circle (really squares to be precise, since the Chebyshev Distance actually is the $L_{\\infty}$)).\n",
    "\n",
    "$$\\text{let} \\quad \\text{M be the monster's position},$$\n",
    "$$\\text{and} \\quad \\psi(P, M) := \n",
    "\\begin{cases} \n",
    "    \\frac{1}{d(P, M)}  & \\text{if } d(P, M) > 0, \\\\\n",
    "    maxcost & \\text{if } d(P, M) = 0\n",
    "\\end{cases}, \\quad \\text{with} \\quad maxcost = 10^{5}$$\n",
    "\n",
    "$$\\text{and} \\quad h(P, T) := d(P, T)$$\n",
    "\n",
    "We can obtain the cost function $f$ designed as described by setting $g = \\psi$.\n",
    "\n",
    "Notice that with $g$ we are comparing our neighbouring position $P$ with the monster position $M$, while with $h$ we are computing the exact distance (thanks to the Chebyshev Distance) between the neighbouring position $P$ and the target $T$.\n",
    "\n",
    "For precisions's sake, we implemented the above function with a slight variation. Basically, we considered two different cases from when the agent knows where the monster is: one where $P$ is a cloud, and another one where $P$ is not a cloud. As one might think, it can be advantageous to exploit clouds in order to run off the monster, but clouds have to be weighted carefully. In fact, we did not want the agent to risk running into the monster just because it is trying to get to a cloud. To achieve this goal, we chose a simple yet effective (at least empirically) approach, based on a very simple manipulation of the above function: \n",
    "\n",
    "$$\\psi(P, M) := \n",
    "\\begin{cases} \n",
    "    \\frac{1}{d(P, M)}  & \\text{if } d(P, M) > 0 \\quad \\text{and } P \\notin Clouds, \\\\\n",
    "    \\left \\lfloor \\frac{1}{d(P, M)} \\right \\rfloor  & \\text{if } d(P, M) > 0 \\quad \\text{and } P \\in Clouds, \\\\\n",
    "    maxcost & \\text{if } d(P, M) = 0\n",
    "\\end{cases}, \\quad \\text{with} \\quad maxcost = 10^{5}$$\n",
    "\n",
    "The intuition behind this, is that the floor gives a very slight advantage to the cloud position in the cost function, precisely an advantage $< 1$. This can be enough given that values will not be too large due to the size of the map, hence we will not need major adjustments.\n",
    "\n",
    "Let us make an observation about what we have seen so far: for now, our approach only targets situations where the agent knows where the monster is located. However, in scenarios where the monster's position is unknown, we need to make some more considerations. Here, the agent should be cautious about approaching clouds, as they are ideal places for monsters to hide. To tackle this issue, we developed a specific function:\n",
    "\n",
    "$$\\varphi(P, Clouds) := \\sum_{C \\in Clouds} \\frac{1}{d(P, C)}$$\n",
    "\n",
    "This function $\\varphi$ can be interpreted as the \"danger function\": it gives a value to represent how \"dangerous\" a position really is, basing the computation on the distance between a given position and each cloud in the map. To achieve this, we again exploited the reciprocal of the Chebyshev Distance, but this time we summed the amount of \"danger\" that each cloud have projected to the specific cell.\n",
    "\n",
    "As before, we can use the original formulation, $f(P, T) := g(P, D) + h(P, T)$, to express the cost function, but this time $g = \\varphi$.\n",
    "\n",
    "$$f(P, T) := \\sum_{C \\in Clouds} \\frac{1}{d(P, C)} + h(P, Q)$$\n",
    "\n",
    "So, we exploited the original formulation of $f$ to handle both scenarios: one where the monster is visible and another where it is not. This was achieved by simply replacing $g$ with the appropriate function for each specific case.\n",
    "\n",
    "Another important observation is that, in the case in which the agent does not know the monster's position, $g$ does not grow linearly as $h$ does (since it is of the form $\\frac{1}{n}$ for the most part), hence this brings the conclusion that $h$ will have a bigger impact on most cases (i.e. each case where $h(P, T) > g(P, M)$, or to put it into words, every time the distance from the target is larger than the reciprocal of the distance from the monster, basically in most occasions). When we consider the case of $g = \\varphi$, the reasoning actually changes a bit, since we are summing over the number of clouds in the map. In any case, this should not affect the algorithm in any particular way.\n",
    "\n",
    "To conclude the discussion, all this machinery can be interpreted as a filtering function: the first filter applies to positions that share the same value of $h$ (since that is the most weighted value), and amongst them the agent chooses the one that is less dangerous to be percolated.\n",
    "\n",
    "A more involved analysis might be interesting to be carried on, but the choice to stop to this point is both of practical meaning and time constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df6daff",
   "metadata": {},
   "source": [
    "However, let us us return to the core issue. \n",
    "As previously mentioned, some monsters in the MiniHack HideNSeek environment are too powerful to face with an empty inventory. Therefore, we have chosen to concentrate on a smaller group, specifically focusing on two monsters: the Naga and the Giant Humanoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9024ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MiniHack-Navigation-Custom-v0\", des_file=\"dat/fully_observable_NH.des\", observation_keys=[\"chars\", \"pixel\", \"colors\"])\n",
    "solve_and_plt(env, heuristic=chebyshev_distance, precision=\"advanced\", plt_width=(100, 270), plt_height=(500, 760), dynamic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a03e324",
   "metadata": {},
   "source": [
    "Let us examine the statistics concerning different the map configurations.\n",
    "\n",
    "In these and the following charts, the label \"unknown\" refers to all the instances where the agent either won or lost without seeing which monster was on the map. In the first case, this probably happened because the agent appeared very close to the stairs and quickly reached them before the monster revealed itself (possibly hiding in the clouds). In the second case, the agent was likely killed by a ranged attack from a hidden monster. Another point to consider is that all the statistics are run over 500 episodes, with a maximum of 30 steps.\n",
    "\n",
    "Initially, let us consider the map where both monsters have an equal probability of appearing.\n",
    "The first histogram represents the total wins and losses: it is clear that the number of victories is significantly higher than the number of defeats, and this proves that our algorithm works in most cases.\n",
    "The second histogram represents the total wins and losses for each type of monster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d26963",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "win, loss, _, monsters_win, monsters_loss = evaluate_performance(\"MiniHack-Navigation-Custom-v0\", dynamic_path_finding, des_file=\"dat/fully_observable_NH.des\", evaluation_steps=500)\n",
    "\n",
    "plot([win, loss], \"1\", 1, [\"Wins\", \"Losses\"], \"Number of matches\", \"Wins and Losses for both Naga and Giant\")\n",
    "\n",
    "plot(monsters_win, \"2\", 0.35, labels, \"Number of matches\", \"Wins and Losses for both Naga and Giant\", monsters_loss, \"Monster type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b76c3a8",
   "metadata": {},
   "source": [
    "Let us have a look at the performance over Naga and the Giant alone.\n",
    "\n",
    "Generally, the win rate is higher in maps featuring only the giant compared to those with only the Naga. This is because, in maps with only the Naga, the agent tends to die more frequently without the monster being visible. This is due to the lethal long-range attack of the Naga, which hits the agent before the monster becomes visible. Additionally, the agent has no defenses against this type of attack, both because it lacks an inventory and because not seeing the monster fails to trigger the logic of the evasion mechanism in the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41937b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAGA\n",
    "win, loss, _, monsters_win, monsters_loss = evaluate_performance(\"MiniHack-Navigation-Custom-v0\", dynamic_path_finding, des_file=\"dat/fully_observable_N.des\", evaluation_steps=500)\n",
    "\n",
    "plot([win, loss], \"1\", 1, [\"Wins\", \"Losses\"], \"Number of matches\", \"Wins and Losses for Naga\")\n",
    "\n",
    "plot(monsters_win, \"2\", 0.45, labels, \"Number of matches\", \"Wins and Losses for Naga\", monsters_loss, \"Monster type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28753d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIANT\n",
    "win, loss, _, monsters_win, monsters_loss = evaluate_performance(\"MiniHack-Navigation-Custom-v0\", dynamic_path_finding, des_file=\"dat/fully_observable_H.des\", evaluation_steps=500)\n",
    "\n",
    "\n",
    "plot([win, loss], \"1\", 1, [\"Wins\", \"Losses\"], \"Number of matches\", \"Wins and Losses for Giant\")\n",
    "\n",
    "plot(monsters_win, \"2\", 0.45, labels, \"Number of matches\", \"Wins and Losses for Giant\", monsters_loss, \"Monster type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f94564",
   "metadata": {},
   "source": [
    "Now, let us have a look at how the algorithm performs on the entire pool of monsters from HideNSeek. \n",
    "\n",
    "The algorithm remains effective even in this case, despite the increased difficulty that facing these monsters without an inventory can pose. In general, win and loss percentages are subject to various influencing factors, including the agent's proximity to the stairs (closer proximity increases the likelihood of reaching them without being killed) and the characteristics of the monster, coupled with its position relative to the agent (for example, favorable outcomes occur when a monster with a potent long-range attack is nearby). These variables are inherently unpredictable. \n",
    "\n",
    "Furthermore, in this specific context, the frequency of a monster's appearance in runs can impact the win rate, a variable beyond our control, as each monster has an equal probability of appearing on the map in every run.\n",
    "Therefore, to comprehensively assess the effectiveness of our algorithm for each type of monster in HideNSeek, a rigorous evaluation would involve running the algorithm on individual maps for each monster type. However, despite this, we can assert the algorithm's general effectiveness based on its performance across 500 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167c245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "win, loss, _, monsters_win, monsters_loss = evaluate_performance(\"MiniHack-HideNSeek-Mapped-v0\", dynamic_path_finding, evaluation_steps=500)\n",
    "\n",
    "\n",
    "plot([win, loss], \"1\", 1, [\"Wins\", \"Losses\"], \"Number of matches\", \"Wins and Losses for all Monsters\")\n",
    "\n",
    "plot(monsters_win, \"2\", 0.35, labels, \"Number of matches\", \"Wins and Losses for all Monsters\", monsters_loss, \"Monster type\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506568cb",
   "metadata": {},
   "source": [
    "Another noteworthy statistic to analyze is the evaluation of the win rate and speed across the different versions of the implemented algorithm.\n",
    "\n",
    "We consider:\n",
    "- the static version (indicated with the label with \"A*\"), where the path is calculated only at the beginning of the episode and remains unchanged;\n",
    "- the initial implementation of the dynamic version (indicated with the label \"dynamic 1\"), where the cost associated with clouds is based solely on the presence or absence of the monster;\n",
    "- the latest dynamic version (indicated with the label \"dynamic 2\"), where the weight of the squares is determined through the calculation explained earlier in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac4b61d",
   "metadata": {},
   "source": [
    "The comparison is carried out on three different maps: one with only Naga, one with only Giant, and one with all the monsters from Hide and Seek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39681b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solo NAGA\n",
    "env = gym.make(\"MiniHack-Navigation-Custom-v0\", des_file=\"dat/fully_observable_N.des\", observation_keys=[\"chars\", \"pixel\", \"colors\"])\n",
    "\n",
    "#STATIC\n",
    "start_time = time.time()\n",
    "win1 = simple_evaluation(env, precision=\"base\", dynamic=False, steps = 500)\n",
    "end_time = time.time()\n",
    "astar = end_time - start_time\n",
    "\n",
    "#DYNAMIC1\n",
    "start_time = time.time()\n",
    "win2 = simple_evaluation(env, precision=\"base\", dynamic=True, steps = 500)\n",
    "end_time = time.time()\n",
    "dynamic1 = end_time - start_time\n",
    "\n",
    "#DYNAMIC2\n",
    "start_time = time.time()\n",
    "win3N = simple_evaluation(env, precision=\"advanced\", dynamic=True, steps = 500)\n",
    "end_time = time.time()\n",
    "dynamic2 = end_time - start_time\n",
    "\n",
    "plot([win1, win2, win3N], \"1\", 0.70, [\"A*\", \"Dynamic 1\", \"Dynamic 2\"], \"Percentage\", \"Comparison of Win Rates Against Naga\", percentage=500)\n",
    "\n",
    "plot([astar, dynamic1, dynamic2], \"1\", 0.70, [\"A*\", \"Dynamic 1\", \"Dynamic 2\"], \"Seconds\", \"Comparison of Execution Time for 500 Episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e8e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solo GIGANTE\n",
    "env = gym.make(\"MiniHack-Navigation-Custom-v0\", des_file=\"dat/fully_observable_H.des\", observation_keys=[\"chars\", \"pixel\", \"colors\"])\n",
    "\n",
    "#STATIC\n",
    "start_time = time.time()\n",
    "win1 = simple_evaluation(env, precision=\"base\", dynamic=False, steps = 500)\n",
    "end_time = time.time()\n",
    "astar = end_time - start_time\n",
    "\n",
    "#DYNAMIC1\n",
    "start_time = time.time()\n",
    "win2 = simple_evaluation(env, precision=\"base\", dynamic=True, steps = 500)\n",
    "end_time = time.time()\n",
    "dynamic1 = end_time - start_time\n",
    "\n",
    "#DYNAMIC2\n",
    "start_time = time.time()\n",
    "win3G = simple_evaluation(env, precision=\"advanced\", dynamic=True, steps = 500)\n",
    "end_time = time.time()\n",
    "dynamic2 = end_time - start_time\n",
    "\n",
    "\n",
    "plot([win1, win2, win3G], \"1\", 0.70, [\"A*\", \"Dynamic 1\", \"Dynamic 2\"], \"Number of matches\", \"Comparison of Number of Games Won Against Giant\", percentage=500)\n",
    "\n",
    "plot([astar, dynamic1, dynamic2], \"1\", 0.70, [\"A*\", \"Dynamic 1\", \"Dynamic 2\"], \"Seconds\", \"Comparison of Execution Time for 500 Episodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cd9fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monster pool\n",
    "env = gym.make(\"MiniHack-HideNSeek-Mapped-v0\", observation_keys=[\"chars\", \"pixel\", \"colors\"])\n",
    "\n",
    "#STATIC\n",
    "start_time = time.time()\n",
    "win1 = simple_evaluation(env, precision=\"base\", dynamic=False, steps = 500)\n",
    "end_time = time.time()\n",
    "astar = end_time - start_time\n",
    "\n",
    "#DYNAMIC1\n",
    "start_time = time.time()\n",
    "win2 = simple_evaluation(env, precision=\"base\", dynamic=True, steps = 500)\n",
    "end_time = time.time()\n",
    "dynamic1 = end_time - start_time\n",
    "\n",
    "#DYNAMIC2\n",
    "start_time = time.time()\n",
    "win3M = simple_evaluation(env, precision=\"advanced\", dynamic=True, steps = 500)\n",
    "end_time = time.time()\n",
    "dynamic2 = end_time - start_time\n",
    "\n",
    "\n",
    "plot([win1, win2, win3M], \"1\", 0.70, [\"A*\", \"Dynamic 1\", \"Dynamic 2\"], \"Number of matches\", \"Comparison of the Number of Games Won Against the Entire Monster Pool\", percentage=500)\n",
    "\n",
    "plot([astar, dynamic1, dynamic2], \"1\", 0.70, [\"A*\", \"Dynamic 1\", \"Dynamic 2\"], \"Seconds\", \"Comparison of Execution Time for 500 Episodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4eeb33",
   "metadata": {},
   "source": [
    "## Test on blend features \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ff1c86",
   "metadata": {},
   "source": [
    "The next step involved combining the two strategies, the knowledge base, and the dynamic pathfinding algorithm, to analyze the obtained results and assess whether this could be a viable strategy.\n",
    "\n",
    "In the following tests, the maximum number of steps for each episode is set to 100. Once that step limit is reached, the episode will conclude with the **noAction** state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b95285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_and_plt(env: gym.Env, heuristic: callable, precision: str, plt_width: Tuple[int, int], plt_height: Tuple[int, int], dynamic: bool = False, suppress = False):\n",
    "    state = env.reset()\n",
    "    game_map = state[\"chars\"]\n",
    "    color_map = state[\"colors\"]\n",
    "    pixel_map = state[\"pixel\"]\n",
    "\n",
    "    start = get_player_location(game_map)\n",
    "    target = get_target_location(game_map)\n",
    "\n",
    "    if dynamic:\n",
    "        dpt_test(game_map, color_map, start, target, env, heuristic, precision=precision, graphics=True, pixel_map=pixel_map, suppress=suppress)\n",
    "    else:\n",
    "        path = a_star(game_map, color_map, start, target, heuristic, precision=precision)\n",
    "        actions = actions_from_path(start, path[1:])\n",
    "        render_actions(actions, env, pixel_map, plt_width, plt_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be264ab8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Test on Naga Blend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0a10b4",
   "metadata": {},
   "source": [
    "In both the case of the Naga and the Giant, we employed the following technique: initially, the agent relies solely on dynamic pathfinding algorithms. As soon as the agent spots the monster, the Knowledge Base comes into play.\n",
    "\n",
    "This technique is effective for the Naga because \n",
    "\n",
    "#TO DO EXPLAIN BETTER BASING THE EXPLANATION ON THE PREVIOUS STATS RESULTS (KB MORE EFFICIENT FOR NAGA) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c32ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NAGA\n",
    "win, loss,noAction, monsters_win, monsters_loss = evaluate_performance(\"MiniHack-Navigation-Custom-v0\", dpt_test, des_file=\"dat/fully_observable_N.des\", evaluation_steps=500, graphics=False)\n",
    "\n",
    "data = [win, loss]\n",
    "x_label = [\"Wins\", \"Losses\"]\n",
    "if noAction:\n",
    "    data.append(noAction)\n",
    "    x_label.append(\"No Action\")\n",
    "\n",
    "plot(data, \"1\", 1, x_label, \"Number of matches\", \"Wins and Losses for Naga\")\n",
    "\n",
    "plot(monsters_win, \"2\", 0.45, labels, \"Number of matches\", \"Wins and Losses for Naga\", monsters_loss, \"Monster type\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc6f216",
   "metadata": {},
   "source": [
    "### Test on Giant blend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6844dd",
   "metadata": {},
   "source": [
    "Initially, the same technique used for the Naga was also applied to address the Giant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ebe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "win, loss,noAction, monsters_win, monsters_loss = evaluate_performance(\"MiniHack-Navigation-Custom-v0\", dpt_test, des_file=\"dat/fully_observable_H.des\", evaluation_steps=500, graphics=False)\n",
    "\n",
    "data = [win, loss]\n",
    "x_label = [\"Wins\", \"Losses\"]\n",
    "if noAction:\n",
    "    data.append(noAction)\n",
    "    x_label.append(\"No Action\")\n",
    "\n",
    "plot(data, \"1\", 1, x_label, \"Number of matches\", \"Wins and Losses for Giant\")\n",
    "\n",
    "plot(monsters_win, \"2\", 0.45, labels, \"Number of matches\", \"Wins and Losses for Giant\", monsters_loss, \"Monster type\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8915f47",
   "metadata": {},
   "source": [
    "However, a different approach was later implemented. In this case, the agent initially relies on dynamic pathfinding. When it detects the presence of the monster, it switches to the Knowledge Base. However, when the agent moves a sufficient distance away from the monster, it reverts back to employing the dynamic pathfinding algorithm.\n",
    "\n",
    "#TO DO EXPLAIN BETTER BASING THE EXPLANATION ON THE PREVIOUS STATS RESULTS (A* MORE EFFICIENT FOR GIANT ????) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b607c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "win, loss,noAction, monsters_win, monsters_loss = evaluate_performance(\"MiniHack-Navigation-Custom-v0\", dpt_testv2, des_file=\"dat/fully_observable_H.des\", evaluation_steps=500, graphics=False)\n",
    "\n",
    "data = [win, loss]\n",
    "x_label = [\"Wins\", \"Losses\"]\n",
    "if noAction:\n",
    "    data.append(noAction)\n",
    "    x_label.append(\"No Action\")\n",
    "\n",
    "plot(data, \"1\", 1, x_label, \"Number of matches\", \"Wins and Losses for all Monsters\")\n",
    "\n",
    "plot(monsters_win, \"2\", 0.45, labels, \"Number of matches\", \"Wins and Losses for all Monsters\", monsters_loss, \"Monster type\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a36b26a",
   "metadata": {},
   "source": [
    "## Comparison Between Knowledge Base, Heuristic Search and Blended Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f002e305",
   "metadata": {},
   "source": [
    "##TO DO (time and win rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5a5130",
   "metadata": {},
   "source": [
    "# The partially observable environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876002c3",
   "metadata": {},
   "source": [
    "In the partially observable environment, an agent's behavior must be adapted to handle the uncertainty and dynamics of the map. The main challenge in this scenario is the changing map configuration in each episode, making it impossible for the agent to rely on pre-existing knowledge or a predetermined path. Additionally, exploration becomes crucial since the stairs, which are the final objective, are not immediately visible.\n",
    "\n",
    "If the stairs are visible, the agent will naturally choose them as the target. Otherwise, the idea is to choose a random position on the frontier as a temporary target. The frontier in this context refers to the limits of the area currently visible to the agent. By moving towards this position, the agent can explore new areas of the map, thereby increasing the likelihood of finding the stairs. Additionally, the algorithm keeps track of positions previously chosen as targets to avoid selecting them again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97f1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MiniHack-Navigation-Custom-v0\", observation_keys=[\"chars\", \"pixel\", \"colors\"], des_file=\"dat/partially_observable_NH.des\")\n",
    "\n",
    "state = env.reset()\n",
    "game_map = state[\"chars\"]\n",
    "color_map = state[\"colors\"]\n",
    "pixel_map = state[\"pixel\"]\n",
    "\n",
    "start = get_player_location(game_map)\n",
    "\n",
    "dynamic_pathfinding_po(game_map, color_map, start, env, chebyshev_distance,\"advanced\", True, True, pixel_map)\n",
    "\n",
    "win, loss, _, monsters_win, monsters_loss = evaluate_performance(\"MiniHack-Navigation-Custom-v0\", dynamic_pathfinding_po, des_file=\"dat/partially_observable_NH.des\", evaluation_steps=300, po = True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3e13c485f7ab2489a53cf312ae0098865ad20bb5d5ebec8b0fad55aa8a342510"
  },
  "kernelspec": {
   "display_name": "Python 3.8.18 ('nle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
